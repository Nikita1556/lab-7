{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237fd82e-724e-41d1-81e4-20100e499920",
   "metadata": {},
   "source": [
    "# Лабораторная 7. Сентимент-анализ\n",
    "\n",
    "Задачи классификации текста аналогично обычной задаче классификации предполагает присвоение метки класса некоторому тексту. Здесь можно действовать любыми методами для того, чтобы классифицировать текст, но мы пойдем по следующему пути: векторизуем последовательности (обязательно почитайте про подходы к векторизации, об эмбеддингах) и обучим RNN\n",
    "\n",
    "После обучения базовых моделей разрешается использовать любой другой подход\n",
    "\n",
    "За выполнение базовой работы можно получить 15 баллов, за преодоление отметки в 94% точности классификации еще 5 баллов\n",
    "Удачи!\n",
    "\n",
    "Примечание: обязательно почитайте про лемматизацию, стеминг, TF-IDF и Word2Vec подходы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377bbbd-cca4-45be-a5d6-2999885bf61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Импорт необходимые библиотеки\n",
    "import pandas as pd  # Для работы с данными в формате таблиц\n",
    "import numpy as np  # Для работы с массивами и математическими функциями\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  # Для улучшенной визуализации данных\n",
    "from tqdm.auto import tqdm  # Для отображения прогресс-баров\n",
    "\n",
    "# Импортирт библиотек для обработки текста\n",
    "import nltk  # Библиотека для работы с текстом\n",
    "from nltk.corpus import stopwords  # Для работы со списком стоп-слов\n",
    "from nltk.stem import WordNetLemmatizer  # Для лемматизации слов\n",
    "import re  # Для работы с регулярными выражениями\n",
    "from collections import Counter  # Для подсчета частоты слов\n",
    "from string import punctuation  # Для работы с пунктуацией\n",
    "\n",
    "# Импорт инструментов для векторизации текста и подготовки данных\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Для TF-IDF векторизации\n",
    "from sklearn.model_selection import train_test_split  # Для разделения данных на обучающую и тестовую выборки\n",
    "from sklearn.preprocessing import LabelEncoder  # Для кодирования меток классов\n",
    "\n",
    "# Импорт Word2Vec для создания векторных представлений слов\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Импорт библиотеки PyTorch для создания и обучения нейронных сетей\n",
    "import torch  \n",
    "import torch.nn as nn  # Для создания нейронных сетей\n",
    "from torch.optim import Adam  # Для оптимизации\n",
    "from torch.utils.data import DataLoader, TensorDataset  # Для работы с данными в формате тензоров\n",
    "\n",
    "# Инициализация лемматизатора и кодировщика меток\n",
    "lemma = WordNetLemmatizer()\n",
    "lb = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79b016-b310-41b8-8a48-85023563689b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Загрузка необходимых ресурсы NLTK\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Загрузка данных из CSV файла\n",
    "df = pd.read_csv('twitter_training.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b5dd3-28b7-4c6c-9bb7-6c2a4f5c3d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Просмотр первых 5 строк данных\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e720b-7b9d-4638-a23a-bf05fe7cc116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Удаление первого столбца\n",
    "df = df.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff77261-3daa-46ac-a45c-a7d5c874b23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Переименовываем столбцы \n",
    "df = df.rename(columns={1: \"Feature2\", 3: \"Feature1\", 2: \"labels\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09cb08-b4ea-48db-ab1e-d4bbd6ac507e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Объединение текстовых данных из двух столбцов в один\n",
    "df[\"tweets\"] = df[\"Feature1\"].astype(str) + \" \" + df[\"Feature2\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df932029-8aa9-4e5a-807b-ed4c731092fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Удаление исходных текстовых столбцов\n",
    "df = df.drop([\"Feature1\", \"Feature2\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4865e27-ddc1-4185-9b9e-a8608659a8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Создание словаря для кодирования меток классов\n",
    "df_labels = {key: value for value, key in enumerate(np.unique(df['labels']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd36ef-dd09-4466-8a13-31a89fdc114d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Проверка уникальных меток классов\n",
    "np.unique(df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c234fc-dae7-40e0-a9d0-aadafb8795fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция для получения числовой метки по тексту\n",
    "def getlabel(n): \n",
    "    for x, y in df_labels.items(): \n",
    "        if y == n: \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c83fa9-5a14-466a-aebd-c1a8ef4ed15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция для предобработки текста\n",
    "def DataPrep(text): \n",
    "    text = re.sub('<.*?>', '', text)  # Удаляем HTML теги\n",
    "    text = re.sub(r'\\d+', '', text)  # Удаляем числа\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаляем специальные символы\n",
    "    text = re.sub(r'http\\S+', '', text)  # Удаляем URL\n",
    "    text = re.sub(r'@\\S+', '', text)  # Удаляем упоминания\n",
    "    text = re.sub(r'#\\S+', '', text)  # Удаляем хештеги\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    \n",
    "    # Удаляем пунктуацию\n",
    "    punc = list(punctuation)\n",
    "    words = [word for word in tokens if word not in punc]\n",
    "    \n",
    "    # Удаление стоп-слова\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word.lower() in stop_words]\n",
    "    \n",
    "    # Лемматизация\n",
    "    words = [lemma.lemmatize(word) for word in words] \n",
    "    \n",
    "    # Возвращение слова обратно в строку\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97998a97-bb73-40a9-b346-238fa9de85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Применение предобработки к каждому твиту\n",
    "df['cleaned_tweets'] = df['tweets'].apply(DataPrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a544e64-ca2b-4733-bdee-52857485ec43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Вывод количества дубликатов\n",
    "print(f'There are around {int(df[\"cleaned_tweets\"].duplicated().sum())} duplicated tweets, we will remove them.')\n",
    "\n",
    "# Удаление дубликатов\n",
    "df.drop_duplicates(\"cleaned_tweets\", inplace=True)\n",
    "\n",
    "# Добавление столбца с длиной твитов\n",
    "df['tweet_len'] = [len(text.split()) for text in df.cleaned_tweets]\n",
    "\n",
    "# Удаление твитов с длиной больше 99.5% квантиля\n",
    "df = df[df['tweet_len'] < df['tweet_len'].quantile(0.995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ba871-5e42-4f6e-bfa2-df3bbae1df9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Визуализация распределения длины твитов\n",
    "plt.figure(figsize=(16, 5))\n",
    "ax = sns.countplot(x='tweet_len', data=df[(df['tweet_len'] <= 1000) & (df['tweet_len'] > 10)], palette='Blues_r')\n",
    "plt.title('Count of tweets with high number of words', fontsize=25)\n",
    "plt.yticks([])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d38d93-ce3b-41b9-b15c-f5a23cff226e",
   "metadata": {},
   "source": [
    "## Обучаем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6956de-978a-4baf-846b-ab6369b08565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Определение максимальной длины твитов\n",
    "MAX_LEN = np.max(df['tweet_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb165366-48d9-4c16-9523-4a268fffdc74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция для подготовки данных для LSTM\n",
    "def lstm_prep(column, seq_len):\n",
    "    # Создаем словарь слов из текстов\n",
    "    corpus = [word for text in column for word in text.split()]\n",
    "    words_count = Counter(corpus)  # Подсчитываем количество слов\n",
    "    sorted_words = words_count.most_common()  # Сортируем слова по частоте\n",
    "    vocab_to_int = {w: i + 1 for i, (w, c) in enumerate(sorted_words)}  # Создаем словарь для преобразования слов в индексы\n",
    "    \n",
    "    text_int = []  # Список для хранения преобразованных текстов\n",
    "    \n",
    "    # Преобразуем каждый текст в последовательность индексов\n",
    "    for text in column:\n",
    "        token = [vocab_to_int[word] for word in text.split()]  # Преобразуем слова в индексы\n",
    "        text_int.append(token)  # Добавляем последовательность в список\n",
    "        \n",
    "    # Подгоняем длину последовательностей\n",
    "    features = np.zeros((len(text_int), seq_len), dtype=int)  # Создаем массив нулей для хранения последовательностей\n",
    "    for idx, y in tqdm(enumerate(text_int)):  # Проходим по всем последовательностям\n",
    "        if len(y) <= seq_len:  # Если длина последовательности меньше или равна максимальной\n",
    "            zeros = list(np.zeros(seq_len - len(y)))  # Создаем список нулей\n",
    "            new = zeros + y  # Добавляем нули в начало последовательности\n",
    "        else:  # Если длина больше максимальной\n",
    "            new = y[:seq_len]  # Обрезаем последовательность до максимальной длины\n",
    "            \n",
    "        features[idx, :] = np.array(new)  # Заполняем массив последовательностями\n",
    "        \n",
    "    return sorted_words, features  # Возвращаем отсортированные слова и массив признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28571c-3aec-4d38-b01c-ccd6f43d2400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Подготавка данных для LSTM\n",
    "VOCAB, tokenized_column = lstm_prep(df['cleaned_tweets'], MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc11ded-cc0a-40df-84e1-bec1e54721ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Вывод первых 10 слов из словаря\n",
    "VOCAB[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811eee9-dffa-4a38-96f4-76d97971267d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Получение размеров токенизированного столбца\n",
    "tokenized_column.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b0be6-eac8-4c99-b48b-47b770eb3720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция для визуализации самых распространенных слов\n",
    "def most_common_words(vocab):\n",
    "    keys = []  # Список для хранения слов\n",
    "    values = []  # Список для хранения их частоты\n",
    "    for key, value in vocab[:30]:  # Проходим по 30 самым распространенным словам\n",
    "        keys.append(key)  # Добавляем слово в список\n",
    "        values.append(value)  # Добавляем частоту в список\n",
    "        \n",
    "    plt.figure(figsize=(15, 5))  # Устанавливаем размер графика\n",
    "    ax = plt.bar(keys, values)  # Строим столбчатую диаграмму\n",
    "    plt.title('Top 20 most common words', size=25)  # Заголовок графика\n",
    "    plt.ylabel(\"Words count\")  # Подпись оси Y\n",
    "    plt.xticks(rotation=45)  # Поворачиваем подписи по оси X\n",
    "    plt.subplots_adjust(bottom=0.15)  # Увеличиваем отступ снизу\n",
    "    plt.show()  # Отображаем график"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44bb059-aaab-4767-9307-7fcb8fd2380d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Визуализируем самые распространенные слова\n",
    "most_common_words(VOCAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099d1d5-c88d-4c49-b0f0-32d421a5c2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Определение X и y для обучения\n",
    "X = tokenized_column\n",
    "y = lb.fit_transform(df['labels'].values)  # Кодируем метки классов\n",
    "\n",
    "# Разделение данных на обучающую и валидационную выборки\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, y, train_size=0.85, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc9f98-8aba-4af7-80c6-f16a71227b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Создание датасета для PyTorch\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.LongTensor(Y_train))\n",
    "val_data = TensorDataset(torch.from_numpy(X_val), torch.LongTensor(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36540bdd-1e95-46ee-895d-51d3826f4f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Устанавливаем размер батча\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Создаем загрузчики данных для обучающей и валидационной выборок\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True  # Перемешиваем данные\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False  # Не перемешиваем валидационные данные\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5a08c-d8de-4172-ad56-4c573ada9189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Устанавливаем размерность эмбеддингов\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164d0ba-a694-4cf9-aca0-c0ed5fee392d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Word2vec_train_data = list(map(lambda x: x.split(), df['cleaned_tweets']))  # Подготовка данные для Word2Vec\n",
    "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)  # Обучение модели Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad971995-31e4-45e6-8cda-fbe9c4952d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция для создания матрицы весов эмбеддингов\n",
    "def weight_matrix(model, vocab):\n",
    "    vocab_size = len(vocab) + 1  # Размер словаря\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))  # Создание матрицы нулей для эмбеддингов\n",
    "    for word, token in vocab:  # Проход по всем словам в словаре\n",
    "        if model.wv.__contains__(word):  # Если слово есть в модели\n",
    "            embedding_matrix[token] = model.wv.__getitem__(word)  # Получение вектор слова и добавляем в матрицу\n",
    "    return embedding_matrix  # Возврат матрицу весов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85a4bf-c99a-4d24-ba23-81b5371608c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Получаем матрицу весов эмбеддингов\n",
    "embedding_vec = weight_matrix(word2vec_model, VOCAB)\n",
    "print(\"Embedding Matrix Shape:\", embedding_vec.shape)  # Вывод форму матрицы весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e15256-74c3-4963-8bb3-da6c9cd7e8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция для подсчета параметров модели\n",
    "def param_count(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]  # Получаем количество параметров, требующих градиента\n",
    "    print('The Total number of parameters in the model : ', sum(params))  # Выводим общее количество параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664ab03-0749-4896-8f15-8d152302cc70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Определяем архитектуру модели\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, hidden_dim, out_channels, bidirectional, device='cpu'):\n",
    "        super().__init__()  # Инициализация родительского класса\n",
    "        print(device)  # Выводим устройство (CPU или GPU)\n",
    "        self.no_layers = num_layers  # Сохраняем количество слоев\n",
    "        self.hidden_dim = hidden_dim  # Сохраняем размер скрытого слоя\n",
    "        self.out_channels = out_channels  # Сохраняем количество выходных каналов\n",
    "        self.num_directions = 2 if bidirectional else 1  # Определяем количество направлений LSTM\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Создаем слой эмбеддингов\n",
    "        self.embedding = self.embedding.to(device)  # Переносим слой на устройство\n",
    "        self.device = device  # Сохраняем устройство\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            dropout=0.5,  # Добавляем дроп-аут для предотвращения переобучения\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True  # Указываем, что первый размер - размер батча\n",
    "        )\n",
    "        self.lstm = self.lstm.to(device)  # Переносим LSTM на устройство\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * self.num_directions, out_channels)  # Полносвязный слой\n",
    "        self.fc = self.fc.to(device)  # Переносим слой на устройство\n",
    "        \n",
    "    # Определяем прямой проход модели\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros((self.no_layers * self.num_directions, x.size(0), self.hidden_dim)).to(self.device)  # Начальное состояние скрытого слоя\n",
    "        c0 = torch.zeros((self.no_layers * self.num_directions, x.size(0), self.hidden_dim)).to(self.device)  # Начальное состояние ячейки\n",
    "        \n",
    "        embedded = self.embedding(x)  # Получаем эмбеддинги для входных данных\n",
    "        \n",
    "        out, _ = self.lstm(embedded, (h0, c0))  # Пропускаем эмбеддинги через LSTM\n",
    "        \n",
    "        out = out[:, -1, :]  # Берем последний выход LSTM для классификации\n",
    "        \n",
    "        out = self.fc(out)  # Пропускаем через полносвязный слой\n",
    "        \n",
    "        return out  # Возвращаем выход модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be721827-b1f7-4987-8b09-9f29cdcb280f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# определение параметров модели\n",
    "VOCAB_SIZE = len(VOCAB) + 1  # Размер словаря\n",
    "NUM_LAYERS = 2  # Количество слоев LSTM\n",
    "OUT_CHANNELS = 4  # Количество выходных классов\n",
    "HIDDEN_DIM = 256  # Размер скрытого слоя\n",
    "BIDIRECTIONAL = True  # Используем двунаправленный LSTM\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Определение устройства\n",
    "# Создание экземпляра модели\n",
    "model = Model(VOCAB_SIZE, EMBEDDING_DIM, NUM_LAYERS, HIDDEN_DIM, OUT_CHANNELS, BIDIRECTIONAL, DEVICE)\n",
    "\n",
    "# Копирование веса эмбеддингов из матрицы весов\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_vec))\n",
    "\n",
    "# Разрешение обновление весов эмбеддингов\n",
    "model.embedding.weight.requires_grad = True\n",
    "\n",
    "# Перенос модели на GPU, если есть возможность\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed9a4f-e5b6-4afa-b994-6c77a086b621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# счетчик параметров модели\n",
    "param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df6e07-513d-4c71-8ece-2f2eb4c3a94c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Определение функцим потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()  # Функция потерь для многоклассовой классификации\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Оптимизатор Adam\n",
    "\n",
    "# Устанавление количества эпох\n",
    "epochs = 10 \n",
    "training_loss = []  # Список для хранения потерь\n",
    "training_acc = []  # Список для хранения точности\n",
    "\n",
    "# Цикл обучения\n",
    "for i in tqdm(range(epochs)):\n",
    "    epoch_loss = 0  # Суммарные потери за эпоху\n",
    "    epoch_acc = 0  # Суммарная точность за эпоху\n",
    "    for batch, (x_train, y_train) in enumerate(train_dataloader):  # Проход по батчам\n",
    "        x_train, y_train = x_train.to(DEVICE), y_train.to(DEVICE)  # Перенос данныч на устройство\n",
    "        y_pred = model(x_train)  # Получаем предсказания модели\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)  # Вычисляем потери\n",
    "        \n",
    "        if batch % 500 == 0:  # Каждые 500 батчей выводим информацию\n",
    "            print(f\"Looked at {batch * len(x_train)}/{len(train_dataloader.dataset)} samples.\")\n",
    "            \n",
    "        loss.backward()  # Вычисление градиентов\n",
    "        optimizer.step()  # Обновление параметров модели\n",
    "        optimizer.zero_grad()  # Обнуление градиентов\n",
    "        \n",
    "        epoch_loss += loss  # Суммирование потери за эпоху\n",
    "        epoch_acc += accuracy_score(y_train.cpu(), y_pred.argmax(dim=1).cpu())  # Вычисляем точность\n",
    "        \n",
    "    # Сохранение потерь и точности за эпоху\n",
    "    training_loss.append((epoch_loss / len(train_dataloader)).detach().cpu().numpy())\n",
    "    training_acc.append(epoch_acc / len(train_dataloader))\n",
    "    \n",
    "    # Вывод результатов обучения за эпоху\n",
    "    print(f\"Epoch {i+1}: Accuracy: {(epoch_acc / len(train_dataloader)) * 100}, Loss: {(epoch_loss / len(train_dataloader))}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6048c-b946-4351-9c6a-04bbd9e980d8",
   "metadata": {},
   "source": [
    "## Время Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad25633-d40c-4499-9081-cae5963f24a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Загружаем тестовый набор данных из CSV файла\n",
    "test_df = pd.read_csv('twitter_validation.csv', header=None)\n",
    "print(test_df.head())  # Выводим первые 5 строк для проверки\n",
    "\n",
    "# Удаляем первый столбец (индексы)\n",
    "test_df = test_df.drop(0, axis=1)\n",
    "\n",
    "# Переименовываем столбцы для удобства\n",
    "test_df = test_df.rename(columns={1: \"Feature2\", 3: \"Feature1\", 2: \"labels\"})\n",
    "test_df.head()  # Проверяем изменения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5e398-a443-46a4-bcdb-c848d628de78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Объединяем тексты из двух столбцов в один столбец \"tweets\"\n",
    "test_df[\"tweets\"] = test_df[\"Feature1\"].astype(str) + \" \" + test_df[\"Feature2\"].astype(str)\n",
    "\n",
    "# Удаляем старые столбцы Feature1 и Feature2\n",
    "test_df = test_df.drop([\"Feature1\", \"Feature2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16d583-63b3-4e18-8f32-beb87b25172f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.head()  # Проверка изменения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b7c6e-21fa-4b56-ac0c-10a016832770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция для выполнения предсказаний на случайных твитах\n",
    "def make_predictions(row):\n",
    "    # Случайным образом выбираем 10 твитов из тестового набора\n",
    "    random_data = row.sample(n=10, random_state=42)  # Установлен random_state для воспроизводимости\n",
    "    random_tweets = random_data['tweets'].values  # Извлекаем тексты твитов\n",
    "    \n",
    "    # Очищаем текст твитов\n",
    "    cleaned_tweets = [DataPrep(tweet) for tweet in random_tweets]  # Используем list comprehension для улучшения читаемости\n",
    "    \n",
    "    # Преобразуем очищенные твиты в векторы\n",
    "    x_test = vec.transform(cleaned_tweets).toarray()  # Преобразуем текст в векторы\n",
    "    \n",
    "    # Извлекаем истинные метки классов\n",
    "    y_test = random_data['labels'].values\n",
    "    \n",
    "    # Подготавливаем данные для LSTM\n",
    "    _, X_test = lstm_prep(cleaned_tweets, MAX_LEN)  # Подготовка данных\n",
    "    X_test = torch.from_numpy(X_test).to(DEVICE)  # Переводим данные в тензор и на нужное устройство\n",
    "\n",
    "    # Получаем предсказания от модели LSTM\n",
    "    lstm_pred = model(X_test)  # Получаем предсказания\n",
    "    lstm_pred = torch.softmax(lstm_pred, dim=1).argmax(dim=1)  # Применяем softmax и получаем индексы классов\n",
    "    \n",
    "    # Получаем метки классов по индексам\n",
    "    pred = np.array([getlabel(lstm_pred[i]) for i in range(len(lstm_pred))])  \n",
    "    \n",
    "    # Выводим оригинальные твиты, их метки и предсказания модели\n",
    "    for i in tqdm(range(2)):  # Ограничиваем вывод первыми двумя твитами\n",
    "        print(f\"The original tweet : {random_tweets[i]}\\n\")  # Выводим оригинальный твит\n",
    "        print(f\"The original label : {y_test[i]}\\n\")  # Выводим истинную метку\n",
    "        print(f\"The lstm prediction is : {getlabel(lstm_pred[i])}\\n\")  # Выводим предсказанную метку\n",
    "        print('-' * 120)  # Разделитель для читабельности\n",
    "\n",
    "    # Вычисляем и выводим точность предсказаний\n",
    "    accuracy = accuracy_score(pred, y_test)  # Вычисляем точность\n",
    "    print(f'Accuracy of predictions: {accuracy:.2f}')  # Выводим точность с двумя знаками после запятой\n",
    "\n",
    "# Вызов функцию для выполнения предсказаний на тестовом наборе данных\n",
    "make_predictions(test_df)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62011a-cd59-442a-8133-4e61b75ecccc",
   "metadata": {},
   "source": [
    "## Что дальше?\n",
    "\n",
    "Попытайтесь улучшить модель (попробуйте GRU), изменить подход к токенизации данных и так далее, удачи!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
